{"nbformat":4,"nbformat_minor":2,"metadata":{"accelerator":"GPU","colab":{"name":"LSTM蒸馏bert.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"code","execution_count":null,"source":["import numpy as np\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","from torchvision import datasets, models, transforms\r\n","import os\r\n","import copy\r\n","from google.colab import drive\r\n","drive.mount('/content/drive/') \r\n","!pip install transformers\r\n","\r\n","data_dir = \"drive/My Drive/模型压缩/hotel\"\r\n","\r\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n","device"],"outputs":[],"metadata":{"id":"F3aQTK8EXYs1"}},{"cell_type":"code","execution_count":null,"source":["# -*- coding: utf-8 -*-\r\n","\r\n","import csv, random\r\n","import torch.nn.functional as F\r\n","from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader\r\n","from transformers import BertModel, BertPreTrainedModel\r\n","from transformers import BertTokenizer\r\n","from transformers import AdamW\r\n","from torch.nn import CrossEntropyLoss\r\n","from tqdm import tqdm, trange\r\n","from sklearn.metrics import f1_score\r\n","from sklearn import metrics\r\n","\r\n","\r\n","class InputExample(object):\r\n","    def __init__(self, text, label=None):\r\n","        self.text  = text\r\n","        self.label = label\r\n","\r\n","\r\n","class InputFeatures(object):\r\n","    def __init__(self, input_ids, input_mask, label_id=None):\r\n","        self.input_ids  = input_ids\r\n","        self.input_mask = input_mask\r\n","        self.label_id   = label_id\r\n","\r\n","\r\n","def create_examples(set_type):\r\n","  examples  = []\r\n","  data_path = data_dir + \"/\" + set_type + \".txt\"\r\n","  with open(data_path, encoding=\"utf-8\") as f:\r\n","    for i, line in enumerate(f):\r\n","        label, text = line.strip().split('\\t', 1)\r\n","        examples.append(InputExample(text=text, label=label))\r\n","  random.shuffle(examples)\r\n","  return examples\r\n","\r\n","\r\n","def convert_examples_to_features(examples, label_list, max_seq, tokenizer):\r\n","    label_map = {label: i for i, label in enumerate(label_list)}\r\n","    features  = []\r\n","    for _, example in enumerate(examples):\r\n","        tokens     = tokenizer.tokenize(example.text)\r\n","        tokens     = [\"[CLS]\"] + tokens[:max_seq - 2] + [\"[SEP]\"]\r\n","        input_ids  = tokenizer.convert_tokens_to_ids(tokens)\r\n","        input_mask = [1] * len(input_ids)\r\n","        padding    = [0] * (max_seq - len(input_ids))\r\n","        label_id   = label_map[example.label]\r\n","        features.append(InputFeatures(\r\n","            input_ids=input_ids + padding,\r\n","            input_mask=input_mask + padding,\r\n","            label_id=label_id))\r\n","    return features\r\n","\r\n","\r\n","class BertClassification(BertPreTrainedModel):\r\n","    def __init__(self, config, num_labels=2):\r\n","        super(BertClassification, self).__init__(config)\r\n","        self.num_labels = num_labels\r\n","        self.bert       = BertModel(config)\r\n","        self.dropout    = nn.Dropout(config.hidden_dropout_prob)\r\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\r\n","        self.init_weights()\r\n","\r\n","    def forward(self, input_ids, input_mask, label_ids):\r\n","        _, pooled_output = self.bert(input_ids, None, input_mask)\r\n","        pooled_output    = self.dropout(pooled_output)\r\n","        logits           = self.classifier(pooled_output)\r\n","        if label_ids is not None:\r\n","            loss_fct = CrossEntropyLoss()\r\n","            return loss_fct(logits.view(-1, self.num_labels), label_ids.view(-1))\r\n","        return logits\r\n","\r\n","\r\n","def createDataset(examples, label_list, max_seq, tokenizer, batch_size):\r\n","  features   = convert_examples_to_features(examples, label_list, max_seq, tokenizer)\r\n","  input_ids  = torch.tensor([f.input_ids  for f in features], dtype=torch.long)\r\n","  input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\r\n","  label_ids  = torch.tensor([f.label_id   for f in features], dtype=torch.long)\r\n","  data       = TensorDataset(input_ids, input_mask, label_ids)\r\n","  sampler    = RandomSampler(data)\r\n","  sets       = DataLoader(data, sampler=sampler, batch_size=batch_size)\r\n","  return sets\r\n","\r\n","\r\n","def compute_metrics(preds, labels):\r\n","    return {'ac': (preds == labels).mean(), 'f1': f1_score(y_true=labels, y_pred=preds)}\r\n","\r\n","\r\n","def fine_tune(bert_model='bert-base-chinese', max_seq=128, batch_size=64, num_epochs=5, lr=2e-5):\r\n","    train_examples  = create_examples(set_type=\"train\")\r\n","    label_list      = ['0', '1']\r\n","    tokenizer       = BertTokenizer.from_pretrained(bert_model, do_lower_case=True)\r\n","    model           = BertClassification.from_pretrained(bert_model, num_labels=len(label_list), return_dict=False)\r\n","    model.to(device)\r\n","    param_optimizer = list(model.named_parameters())\r\n","    no_decay        = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\r\n","    optimizer_grouped_parameters = [\r\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\r\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.00}]\r\n","    optimizer        = AdamW(optimizer_grouped_parameters, lr=lr)\r\n","\r\n","    print('train...')\r\n","    train_dataloader = createDataset(train_examples, label_list, max_seq, tokenizer, batch_size)\r\n","    model.train()\r\n","    for _ in trange(num_epochs, desc='Epoch'):\r\n","        tr_loss = 0\r\n","        for step, batch in enumerate(tqdm(train_dataloader, desc='Iteration')):\r\n","            input_ids, input_mask, label_ids = tuple(t.to(device) for t in batch)\r\n","            loss = model(input_ids, input_mask, label_ids)\r\n","            loss.backward()\r\n","            optimizer.step()\r\n","            optimizer.zero_grad()\r\n","            tr_loss += loss.item()\r\n","        print('tr_loss', tr_loss)\r\n","\r\n","        print('eval...')\r\n","        eval_examples = create_examples(set_type=\"train\")\r\n","        eval_dataloader = createDataset(eval_examples, label_list, max_seq, tokenizer, batch_size)\r\n","        model.eval()\r\n","        all_predictions, all_labels = [], []\r\n","        for batch in tqdm(eval_dataloader, desc='Evaluating'):\r\n","            input_ids, input_mask, label_ids = tuple(t.to(device) for t in batch)\r\n","            with torch.no_grad():\r\n","                logits = model(input_ids, input_mask, None)\r\n","                preds = logits.detach().cpu().numpy()\r\n","                preds = np.argmax(np.vstack(preds), axis=1)\r\n","                # print(compute_metrics(preds, label_ids.cpu().numpy()))\r\n","                all_predictions.extend(preds)\r\n","                all_labels.extend(label_ids.cpu().numpy())\r\n","                # 计算auc\r\n","        fpr, tpr, thresholds = metrics.roc_curve(y_true=all_labels,\r\n","                                                y_score=all_predictions)\r\n","        auc = metrics.auc(fpr, tpr)\r\n","        print(\"AUC: \",  auc)\r\n","    # torch.save(model, 'drive/My Drive/模型压缩/test_model')\r\n","\r\n","fine_tune()"],"outputs":[],"metadata":{"id":"CocAyPYvX230"}},{"cell_type":"code","execution_count":null,"source":["class Teacher(object):\r\n","    def __init__(self, bert_model='bert-base-chinese', max_seq=128):\r\n","        self.max_seq = max_seq\r\n","        self.tokenizer = BertTokenizer.from_pretrained(\r\n","            bert_model, do_lower_case=True)\r\n","        self.model = torch.load('drive/My Drive/模型压缩/test_model')\r\n","        self.model.eval()\r\n","\r\n","    def predict(self, text):\r\n","        tokens = self.tokenizer.tokenize(text)[:self.max_seq]\r\n","        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\r\n","        input_mask = [1] * len(input_ids)\r\n","        padding = [0] * (self.max_seq - len(input_ids))\r\n","        input_ids = torch.tensor([input_ids + padding], dtype=torch.long).to(device)\r\n","        input_mask = torch.tensor([input_mask + padding], dtype=torch.long).to(device)\r\n","        logits = self.model(input_ids, input_mask, None)\r\n","        # print(logits)\r\n","        return F.softmax(logits, dim=1).detach().cpu().numpy()\r\n"],"outputs":[],"metadata":{"id":"hSmgxWLgX276"}},{"cell_type":"code","execution_count":null,"source":["import jieba, fileinput\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing import sequence\r\n","\r\n","def load_part_set(path, name, tokenizer):\r\n","  X, Y = [], []\r\n","  TEXT = []\r\n","  data_path = \"drive/My Drive/模型压缩/\" + name + \"/\" + path + \".txt\"\r\n","  for line in open(data_path, encoding=\"utf-8\").read().strip().split('\\n'):\r\n","        label, text = line.split('\\t', 1)\r\n","        TEXT.append(text.strip())\r\n","        X.append(' '.join(jieba.cut(text.strip())))\r\n","        Y.append(int(label))\r\n","  X = tokenizer.texts_to_sequences(X)\r\n","  return X, Y, TEXT\r\n","\r\n","\r\n","def load_data(name):\r\n","    def get_w2v():\r\n","        for line in open('drive/My Drive/word2vec', encoding=\"utf-8\").read().strip().split('\\n'):\r\n","            line = line.strip().split()\r\n","            if not line: continue\r\n","            yield line[0], np.array(list(map(float, line[1:])))\r\n","\r\n","    tokenizer = Tokenizer(filters='', lower=True, split=' ', oov_token=1)\r\n","    texts = [' '.join(jieba.cut(line.split('\\t', 1)[1].strip())) \\\r\n","             for line in open('drive/My Drive/模型压缩/{}/{}.txt'.format(name, name), encoding=\"utf-8\",\r\n","                              ).read().strip().split('\\n')]\r\n","    tokenizer.fit_on_texts(texts)\r\n","    \r\n","    x_train, y_train, text_train = load_part_set(\"train\", name, tokenizer)\r\n","    x_dev,   y_dev,   text_dev   = load_part_set(\"eval\",   name, tokenizer)\r\n","    x_test,  y_test,  text_test  = load_part_set(\"test\",  name, tokenizer)\r\n","\r\n","    v_size = len(tokenizer.word_index) + 1\r\n","    embs, w2v = np.zeros((v_size, 300)), dict(get_w2v())\r\n","    for word, index in tokenizer.word_index.items():\r\n","        if word in w2v: embs[index] = w2v[word]\r\n","    return (x_train, y_train, text_train), \\\r\n","           (x_dev, y_dev, text_dev), \\\r\n","           (x_test, y_test, text_test), \\\r\n","           v_size, embs\r\n","\r\n"],"outputs":[],"metadata":{"id":"LhVY4ozf_i0x"}},{"cell_type":"code","execution_count":null,"source":["from tqdm import tqdm\r\n","\r\n","teacher = Teacher()\r\n","\r\n","x_len      = 50\r\n","lr         = 0.001\r\n","epochs     = 10\r\n","name       = 'hotel'  \r\n","\r\n","\r\n","(x_train, y_train, T_train), (x_val, y_val, T_val), (x_test, y_test, T_test), vec_size, embs = load_data(name)\r\n","\r\n","x_train = sequence.pad_sequences(x_train, maxlen=x_len)\r\n","x_val   = sequence.pad_sequences(x_val,   maxlen=x_len)\r\n","x_test  = sequence.pad_sequences(x_test,  maxlen=x_len)\r\n","\r\n","with torch.no_grad():\r\n","    teacher_train = np.vstack([teacher.predict(text) for text in tqdm(T_train)])\r\n","    teacher_val   = np.vstack([teacher.predict(text) for text in tqdm(T_val[:2000])])\r\n"],"outputs":[],"metadata":{"id":"yVk_A5Z1_YOA"}},{"cell_type":"code","execution_count":null,"source":["class RNN(nn.Module):\r\n","    def __init__(self, x_dim, e_dim, h_dim, o_dim):\r\n","        super(RNN, self).__init__()\r\n","        self.h_dim       = h_dim\r\n","        self.dropout     = nn.Dropout(0.2)\r\n","        self.emb         = nn.Embedding(x_dim, e_dim, padding_idx=0)\r\n","        self.lstm        = nn.LSTM(e_dim, h_dim, bidirectional=True, batch_first=True)\r\n","        self.fc          = nn.Linear(h_dim * 2, o_dim)\r\n","        self.softmax     = nn.Softmax(dim=1)\r\n","        self.log_softmax = nn.LogSoftmax(dim=1)\r\n","\r\n","    def forward(self, x):\r\n","        embed  = self.dropout(self.emb(x))\r\n","        out, _ = self.lstm(embed)\r\n","        hidden = self.fc(out[:, -1, :])\r\n","        return self.softmax(hidden), self.log_softmax(hidden)"],"outputs":[],"metadata":{"id":"Mz3_FYeZIPyg"}},{"cell_type":"code","execution_count":null,"source":["from torch.autograd import Variable\r\n","from keras.preprocessing import sequence\r\n","\r\n","LTensor = torch.cuda.LongTensor \r\n","FTensor = torch.cuda.FloatTensor \r\n","\r\n","model    = RNN(vec_size, 256, 256, 2)\r\n","model    = model.cuda()\r\n","opt      = optim.Adam(model.parameters(), lr=lr)\r\n","ce_loss  = nn.NLLLoss()\r\n","mse_loss = nn.MSELoss()\r\n","batch_size = 64\r\n","for epoch in range(epochs):\r\n","    losses = []\r\n","    accu   = []  \r\n","\r\n","    model.train()\r\n","    for i in range(0, len(x_train), batch_size):\r\n","            model.zero_grad()\r\n","            student = Variable(LTensor(x_train[i:i + batch_size]))\r\n","            teacher = Variable(FTensor(teacher_train[i:i + batch_size]))\r\n","            pred1, pred2 = model(student)\r\n","            loss = mse_loss(pred1, teacher)\r\n","            # by = Variable(LTensor(y_de[i:i + b_size]))\r\n","            # loss = alpha * ce_loss(py2, by) + (1-alpha) * mse_loss(py1, bt)\r\n","            loss.backward()             \r\n","            opt.step()                      \r\n","            losses.append(loss.item())\r\n","    model.eval()\r\n","\r\n","    with torch.no_grad():\r\n","        for i in range(0, len(x_val), batch_size):\r\n","            input = Variable(LTensor(x_val[i:i + batch_size]))\r\n","            label = Variable(LTensor(y_val[i:i + batch_size]))\r\n","            _, pred = torch.max(model(input)[1], 1)\r\n","            fpr, tpr, thresholds = metrics.roc_curve(y_true=label.cpu(), y_score=pred.cpu())\r\n","            auc = metrics.auc(fpr, tpr)\r\n","            print(\"AUC: \",  auc)\r\n","            accu.append((pred == label).float().mean().item())\r\n","    print(\"loss: \", np.mean(losses),  \"  acc\",  np.mean(accu))"],"outputs":[],"metadata":{"id":"tBeTmuATIMLn"}},{"cell_type":"code","execution_count":null,"source":["torch.save(model, \"drive/My Drive/模型压缩/BiLSTM\")"],"outputs":[],"metadata":{"id":"M8P5SUvwFpzE"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"30Mc8iuv1lye"}}]}